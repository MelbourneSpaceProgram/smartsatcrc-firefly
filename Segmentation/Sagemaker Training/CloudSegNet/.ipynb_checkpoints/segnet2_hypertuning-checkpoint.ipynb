{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegNet2 Hyperparamter Tuning\n",
    "* This marks the first attempt at utilising the hyperparameter tuning in sagemaker\n",
    "* The following guide was used: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist.ipynb\n",
    "* The first attempts did not work due to the limited memory of the instance types that were available to us at the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set Up ################\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# ##yan - training on local instance to see what the problem is  \n",
    "# import boto3\n",
    "# import os\n",
    "# from sagemaker.local import LocalSession\n",
    "# from sagemaker.debugger import rule_configs, DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "# sagemaker_session = LocalSession()\n",
    "# sagemaker_session.config = {'local': {'local_code': True}}\n",
    "# # Make sure to set this to your bucket and location\n",
    "# BUCKET_NAME = 'sagemaker-firefly-model-artifacts'\n",
    "# LOCATION_IN_BUCKET = 'smdebug_debug'\n",
    "\n",
    "# s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET)\n",
    "# ##\n",
    "\n",
    "sagemaker_session = sagemaker.Session() #use for remote session\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "# prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Path Set Up #################\n",
    "# speciy location of training data\n",
    "# train_data = 's3://sagemaker-firefly-model-data/Cloud Segmentation Data/38-Cloud_training'.format(bucket, prefix, 'train')\n",
    "train_data = 's3://sagemaker-firefly-model-data/Cloud Segmentation Data/38-Cloud_training'\n",
    "# train_data=\"file:///home/ec2-user/SageMaker/Dataset/CloudSeg/38-Cloud_training\"\n",
    "# validation_data = 's3://{}/{}/{}'.format(bucket, prefix, 'validation') currently training script does this split\n",
    "\n",
    "# specifiy location for model output to be saved\n",
    "# s3_output_location = 's3://sagemaker-firefly-model-artifacts'.format(bucket, prefix, 'xgboost_model_sdk')\n",
    "s3_output_location = 's3://sagemaker-firefly-model-artifacts/segnet2_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::067338613469:role/service-role/AmazonSageMaker-ExecutionRole-20200914T180083'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialise the PyTorch training estimator object ###########\n",
    "\n",
    "# train_instance_type='ml.c4.4xlarge'\n",
    "# estimator = PyTorch(entry_point='segnet2_train_script.py',\n",
    "#                     role=role,\n",
    "#                     framework_version='1.4.0',\n",
    "#                     train_instance_count=1,\n",
    "#                     train_instance_type='local',\n",
    "#                     output_path=s3_output_location,\n",
    "#                     hyperparameters={\n",
    "#                         'epochs': 5,\n",
    "#                         'learning_rate':0.001,\n",
    "#                         'batch-size':8\n",
    "#                     },\n",
    "#                     debugger_hook_config = DebuggerHookConfig(\n",
    "#                         s3_output_path=s3_bucket_for_tensors,  # Required\n",
    "#                         collection_configs=[\n",
    "#                             CollectionConfig(\n",
    "#                                 name=\"conv0_tensors\",\n",
    "#                                 parameters={\n",
    "#                                     \"include_regex\": \"*\",\n",
    "#                                     \"save_interval\": \"100\"\n",
    "#                                 }\n",
    "#                             )\n",
    "#                         ]\n",
    "#                     )\n",
    "#                    )\n",
    "estimator = PyTorch(entry_point='segnet2_hypertuning_train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=2,\n",
    "                    # train_use_spot_instances = True, account not allowed\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    output_path=s3_output_location,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 15,\n",
    "                        'backend': 'gloo',\n",
    "                        'batch_size': 8\n",
    "                    }\n",
    "                   )\n",
    "\n",
    "# add train_use_spot_instances = True for spot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.001, 0.1)\n",
    "    }\n",
    "# turns out that a batch size of 32 was too much for the GPU\n",
    "# changed to multiple's of 4 instead of powers of 2 to provide the same number of options\n",
    "\n",
    "# batch size note tested, as the training instance did not have enough memory to handle decently sized batch sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'average test loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': train_data}, wait=False, logs='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "* The hypertuning job failed with the cuase being \"CUDA out of memory\"\n",
    "* We requrie larger instances\n",
    "* Current quotas bar us from accessing any of the larger instances\n",
    "* In the process of requresting for more, have sent email and waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Attempt\n",
    "* The quotas have been passed\n",
    "* Now the hypertuning will be tried again\n",
    "* The purpose of this is to test the hypertuning funciton before it is applied to the smoke segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Initialise the PyTorch training estimator object ###########\n",
    "\n",
    "estimator = PyTorch(entry_point='segnet2_hypertuning_train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    # train_use_spot_instances = True, account not allowed\n",
    "                    train_instance_type='ml.p3.2xlarge',\n",
    "                    output_path=s3_output_location,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 15,\n",
    "                        'backend': 'gloo',\n",
    "                    }\n",
    "                   )\n",
    "\n",
    "# add train_use_spot_instances = True for spot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyperparaters and ranges to be tested\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.001, 0.1),\n",
    "    'batch_size': CategoricalParameter([4,8,16,32])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required set up for hypertuning\n",
    "objective_metric_name = 'average test loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the hyperparameter tuning object\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# run the tuning\n",
    "tuner.fit({'train': train_data}, wait=False, logs='All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
